{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вручтель Серафима. 495 группа. Знакомство с линейным классификатором."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Бинарный линейный классификатор (формула для отображения из множества объектов в множество классов):\n",
    "    \n",
    "$a(x) = sign (<w, x> - w_0)$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Отступ алгоритма $a(x)$ на объекте $x_i$ имеет вид:\n",
    "\n",
    "$M_i = y_i \\cdot f(x_i)$, где $f(x) = <w, x> - w_0$, $y_i$ -класс, к которму относится $x_i$.\n",
    "\n",
    "Если $M_i \\le 0$, то реальный класс не совпал с ответом алгоритма: $y_i \\ne a(x_i)$,\n",
    "\n",
    "если $M_i \\ge 0$, то реальный класс совпал с ответом алгоритма: $y_i = a(x_i)$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Для сведения классификатора вида $a(x) = sign (<w, x> - w_0)$ к классификатору вида $a(x) = sign (<w, x>)$ к вектору $x$ добавляют фиктивную координату $x_0 = -1$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Функционал эмпирического риска через отступы выглядит следующим образом:\n",
    "\n",
    "$Q(w) = \\Sigma_{i = 1}^l [M_i(w) < 0]$ . \n",
    "\n",
    "Этот функционал не является гладким, его можно аппроксмировать функционалом вида\n",
    "\n",
    "$Q(w) = \\Sigma_{i = 1}^l L(M_i(w))$ , где $L(x)$ - некоторая гладкая функция.\n",
    "\n",
    "Для \"наилучшего\" алгоритма классификации этот функционал необходимо минимизировать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Если в функционале эмпирического риска с пороговой функцией потерь всюду записаны строгие неравенства: $Q(w) = \\Sigma_{i = 1}^l [M_i(w) < 0]$, то параметр $w = 0$ минимизирует такой функционал:\n",
    "\n",
    "$w = 0 \\Rightarrow a(x) = 0 \\Rightarrow M_i(w) = 0 \\ \\forall \\ i \\Rightarrow Q(w) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Функция потерь $L(M)$ имеет вид $L(M) = \\log _2 (1 + e^{-M})$, а функционал эмпирического риска с такой функцией потерь:\n",
    "\n",
    "$Q(w) = \\Sigma_{i = 1}^l \\log _2 (1 + e^{-M_i})$.\n",
    "\n",
    "(В общем случае: $Q(w) = \\Sigma_{i = 1}^l L(M_i(w))$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Функция потерь - мера расхождения между истинным классом и ответом алгоритма. Минимизируя функцию потерь можно получить оптимальный алгоритм классификации. График функции потерь обычно убывает с ростом параметра."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Пример негладкой функции потерь: $[M_i(w) < 0]$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Регуляризация - метод добавления некоторой дополнительной информации к условию с целью предотвращения переобучения. Идейно - это добавление некоторых ограничений или штрафов на рост коэффициентов в алгоритме.\n",
    "\n",
    "Известные регуляризаторы: $l_1$ - с ограничением на сумму модулей весов, $l_2$ - с ограничением на сумму квадратов весов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) Обобщающая способность алгоритма заключается в предсказуемости ошибки на тестовой выборке: она должна не сильно отличаться от ошибки на трейновой выборке. Чем ниже обобщающая способность алгоритма, тем выше его склонность к переобучению.\n",
    "\n",
    "Регуляризация положительно сказывается на обобщающей способности алгоритма, так как не даёт ему переобучиться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) Острые минимумы функционала аппроксимированного эмпирического риска свидетельтсвуют о переобучении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) При добавлении регуляризации к аппроксимированному риску с соответствующими весами как функции от параметров алгоритма появляется необходимость минимизировать полученную сумму. Чем больше веса, тем больше регуляризатор, следовательно, получается штраф за излишний прирост значений параметров алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13) Функционал аппроксимированного риска будет принимать большее значение на обучающей выборке в случае построения с регуляризацией, так как в этом случае в функционал добавляется регуляризатор с некоторым положительным весом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14) Функционал риска будет принимать большее значение на тестовой выборке для алгоритма классификации, построенного с оправдывающей себя регуляризацией, так как будет меньше переобучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15) Accuracy - доля правильных ответов при классификации: $Accuracy = \\frac{T}{N}$, где $N$ - количество всех ответов.\n",
    "\n",
    "Precision (точность) - в пределах класса это доля элементов действительно принадлежащих к данному классу относительно всех элементов, которые алгоритм отнёс к данному классу.\n",
    "\n",
    "Recall (полнота) - доля найденных классификатором элементов, принадлежащих классу, относительно всех элементов класса в тестовой выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16) ROC-кривая - график, отражающий соотношение между долей верных положительных классификаций и долей ложных положительных классификаций при варьировании порога решающего правила (случай вероятностной классификации) - зависимость TPR от FPR.\n",
    "\n",
    "AUC - площадь под графиком ROC-кривой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17) Алгоритм построения ROC-кривой.\n",
    "\n",
    "Пусть есть два класса $+1$ и $-1$, в исходной выборке $m$ элементов.\n",
    "\n",
    "- Вычислим количество представителей классов $+1$ и $-1$ в выборке: $m_- = \\Sigma_{i = 1}^{m} [y_i = -1]$, $m_+ = \\Sigma_{i = 1}^{m} [y_i = +1]$ .\n",
    "\n",
    "- Вычислим ответы алгоритма $a(x_i)$ на исходной выборке и упорядочим выборку по убыванию этих ответов.\n",
    "\n",
    "- Установим начальную точку ROC-кривой: $(FPR_0, TPR_0) = (0, 0)$ .\n",
    "\n",
    "- Для всех элементов выборки $(i = 1, ..., m)$:\n",
    "\n",
    "если $y_i = -1$, то сместиться на 1 шаг вправо $FPR_i = FPR_{i - 1} + \\frac{1}{m_-}$, $TPR_i = TPR_{i - 1}$,\n",
    "\n",
    "иначе - сместиться на 1 шаг вверх $FPR_i = FPR_{i - 1}$, $TPR_i = TPR_{i - 1} + \\frac{1}{m_+}$ ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
